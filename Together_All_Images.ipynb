{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeRocksDataset:\n",
    "    def __init__(self, image_folder: str, json_dataset: str, output_path: str, dsm_folder: str, hillshade_folder: str):\n",
    "        \"\"\"\n",
    "        Initialize the dataloader with additional modalities.\n",
    "        \n",
    "        Args:\n",
    "            image_folder (str): Path to folder containing RGB images.\n",
    "            json_dataset (str): Path to JSON dataset file.\n",
    "            output_path (str): Path to save YOLOv8 formatted dataset.\n",
    "            dsm_folder (str): Path to folder containing DSM images.\n",
    "            hillshade_folder (str): Path to folder containing Hillshade images.\n",
    "        \"\"\"\n",
    "        self.image_folder = image_folder\n",
    "        self.dsm_folder = dsm_folder\n",
    "        self.hillshade_folder = hillshade_folder\n",
    "        self.label_file = json_dataset\n",
    "        self.output_path = output_path\n",
    "\n",
    "        # Define directories for splits\n",
    "        self.splits = [\"train\", \"val\", \"test\"]\n",
    "        self.image_dir = os.path.join(output_path, \"images\")\n",
    "        self.dsm_dir = os.path.join(output_path, \"dsm\")\n",
    "        self.hillshade_dir = os.path.join(output_path, \"hillshade\")\n",
    "        self.label_dir = os.path.join(output_path, \"labels\")\n",
    "\n",
    "        # Create directories for each split\n",
    "        for split in self.splits:\n",
    "            os.makedirs(os.path.join(self.image_dir, split), exist_ok=True)\n",
    "            os.makedirs(os.path.join(self.dsm_dir, split), exist_ok=True)\n",
    "            os.makedirs(os.path.join(self.hillshade_dir, split), exist_ok=True)\n",
    "            os.makedirs(os.path.join(self.label_dir, split), exist_ok=True)\n",
    "\n",
    "    def _convert_bbox(self, rel_loc: Tuple[float, float], bbox_size: Tuple[int, int], img_size: Tuple[int, int]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Convert bounding box info to YOLO format: [class_id, x_center, y_center, width, height]. (Only one class for rocks)\n",
    "        \"\"\"\n",
    "        x_center, y_center = rel_loc\n",
    "        width = bbox_size[0] / img_size[0]\n",
    "        height = bbox_size[1] / img_size[1]\n",
    "        return [0, x_center, y_center, width, height]  # class_id = 0 for rocks\n",
    "\n",
    "    def process_dataset(self):\n",
    "        \"\"\"\n",
    "        Process the dataset + convert it to YOLOv8 format with train/val/test splits.\n",
    "        \"\"\"\n",
    "        # Load the annotations JSON\n",
    "        with open(self.label_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Iterate over each image in the dataset\n",
    "        for tile in data['dataset']:\n",
    "            file_name = tile['file_name']\n",
    "            img_path = os.path.join(self.image_folder, file_name)\n",
    "            dsm_path = os.path.join(self.dsm_folder, file_name)\n",
    "            hillshade_path = os.path.join(self.hillshade_folder, file_name)\n",
    "\n",
    "            # Check if the RGB, DSM, and Hillshade images exist\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Image {img_path} not found. Skipping.\")\n",
    "                continue\n",
    "            if not os.path.exists(dsm_path):\n",
    "                print(f\"DSM {dsm_path} not found. Skipping.\")\n",
    "                continue\n",
    "            if not os.path.exists(hillshade_path):\n",
    "                print(f\"Hillshade {hillshade_path} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            img_width, img_height = tile['width'], tile['height']\n",
    "            annotations = tile.get('rocks_annotations', [])\n",
    "            split = tile.get('split', \"train\")  # Default to 'train' if no split is specified\n",
    "\n",
    "            # Copy the images to the appropriate YOLO folders\n",
    "            shutil.copy(img_path, os.path.join(self.image_dir, split, file_name))\n",
    "            shutil.copy(dsm_path, os.path.join(self.dsm_dir, split, file_name))\n",
    "            shutil.copy(hillshade_path, os.path.join(self.hillshade_dir, split, file_name))\n",
    "\n",
    "            # Prepare labels for this image\n",
    "            label_lines = []\n",
    "            for annotation in annotations:\n",
    "                rel_loc = annotation['relative_within_patch_location']\n",
    "                bbox_size = annotation.get('bbox_size', [30, 30])  # Default to 30x30 if bbox size is missing\n",
    "                yolo_bbox = self._convert_bbox(rel_loc, bbox_size, (img_width, img_height))\n",
    "                label_lines.append(\" \".join(map(str, yolo_bbox)))\n",
    "\n",
    "            # Save labels to the appropriate folder\n",
    "            label_file = os.path.join(self.label_dir, split, f\"{os.path.splitext(file_name)[0]}.txt\")\n",
    "            with open(label_file, 'w') as lf:\n",
    "                lf.write(\"\\n\".join(label_lines))\n",
    "\n",
    "        print(f\"Dataset processed and saved to {self.output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset processed and saved to YOLO\n"
     ]
    }
   ],
   "source": [
    "# Paths to your data\n",
    "rgb_folder = \"swissImage_50cm_patches\"\n",
    "dsm_folder = \"swissSURFACE3D_patches\"\n",
    "hillshade_folder = \"swissSURFACE3D_hillshade_patches\"\n",
    "label_file = \"large_rock_dataset.json\"\n",
    "output_path = \"YOLO\"\n",
    "\n",
    "# Create and process the dataset\n",
    "rocks_dataset = LargeRocksDataset(rgb_folder, label_file, output_path, dsm_folder, hillshade_folder)\n",
    "rocks_dataset.process_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, rgb_dir, dsm_dir, hillshade_dir, label_dir, geom_transform=None, rgb_transform=None, all_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb_dir (str): Directory containing RGB images.\n",
    "            dsm_dir (str): Directory containing DSM images.\n",
    "            hillshade_dir (str): Directory containing Hillshade images.\n",
    "            label_dir (str): Directory containing YOLO-style labels.\n",
    "            geom_transform (callable): Geometric transforms applied to all modalities (e.g., flipping, rotation).\n",
    "            rgb_transform (callable): Pixel-value transforms applied only to RGB images (e.g., brightness, contrast).\n",
    "            all_transform (callable): Transform applied to all modalities (e.g., resizing).\n",
    "        \"\"\"\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.dsm_dir = dsm_dir\n",
    "        self.hillshade_dir = hillshade_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.geom_transform = geom_transform\n",
    "        self.rgb_transform = rgb_transform\n",
    "        self.all_transform = all_transform\n",
    "        self.image_files = sorted(os.listdir(rgb_dir))\n",
    "        self.label_files = sorted(os.listdir(label_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load RGB, DSM, and Hillshade images\n",
    "        rgb_path = os.path.join(self.rgb_dir, self.image_files[index])\n",
    "        dsm_path = os.path.join(self.dsm_dir, self.image_files[index])\n",
    "        hillshade_path = os.path.join(self.hillshade_dir, self.image_files[index])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[index])\n",
    "        \n",
    "        rgb = Image.open(rgb_path).convert(\"RGB\")\n",
    "        dsm = Image.open(dsm_path).convert(\"L\")\n",
    "        hillshade = Image.open(hillshade_path).convert(\"L\")\n",
    "\n",
    "        # Load labels\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = f.readlines()\n",
    "        labels = [list(map(float, line.strip().split())) for line in labels]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Apply geometric transforms\n",
    "        if self.geom_transform:\n",
    "            combined_image = {\"rgb\": rgb, \"dsm\": dsm, \"hillshade\": hillshade}\n",
    "            combined_image, labels = self.geom_transform(combined_image, labels)\n",
    "            rgb, dsm, hillshade = combined_image[\"rgb\"], combined_image[\"dsm\"], combined_image[\"hillshade\"]\n",
    "\n",
    "        # Apply RGB-specific pixel-value transforms\n",
    "        if self.rgb_transform:\n",
    "            rgb = self.rgb_transform(rgb)\n",
    "\n",
    "        # Apply general transforms to all modalities (e.g., resizing, normalization)\n",
    "        if self.all_transform:\n",
    "            rgb = self.all_transform(rgb)\n",
    "            dsm = self.all_transform(dsm)\n",
    "            hillshade = self.all_transform(hillshade)\n",
    "\n",
    "        # Convert images to numpy arrays and stack them\n",
    "        rgb = np.array(rgb)\n",
    "        dsm = np.expand_dims(np.array(dsm), axis=-1)\n",
    "        hillshade = np.expand_dims(np.array(hillshade), axis=-1)\n",
    "        combined = np.concatenate((rgb, dsm, hillshade), axis=-1)  # Shape: (H, W, 5)\n",
    "        combined = combined.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        combined = torch.tensor(combined).permute(2, 0, 1)  # Shape: (5, H, W)\n",
    "\n",
    "        return combined, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlipWithBBox:\n",
    "    def __init__(self, flip_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the transform with a probability of flipping the image and bounding boxes.\n",
    "        Args:\n",
    "            flip_prob (float): Probability of applying the horizontal flip.\n",
    "        \"\"\"\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, labels):\n",
    "        \"\"\"\n",
    "        Apply the transformation.\n",
    "        Args:\n",
    "            image (PIL.Image): The input image.\n",
    "            labels (torch.Tensor): The bounding box labels in YOLO format (class, x_center, y_center, width, height).\n",
    "        Returns:\n",
    "            image (PIL.Image): Transformed image.\n",
    "            labels (torch.Tensor): Adjusted bounding box labels.\n",
    "        \"\"\"\n",
    "        if random.random() < self.flip_prob:\n",
    "            # Flip the image horizontally\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            # Adjust the labels\n",
    "            if len(labels) > 0:\n",
    "                labels[:, 1] = 1 - labels[:, 1]  # Invert the x_center for horizontal flip\n",
    "        \n",
    "        return image, labels\n",
    "    \n",
    "\n",
    "class RandomVerticalFlipWithBBox:\n",
    "    def __init__(self, flip_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the transformation with a probability of flipping vertically.\n",
    "        Args:\n",
    "            flip_prob (float): Probability of applying the vertical flip.\n",
    "        \"\"\"\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, labels):\n",
    "        \"\"\"\n",
    "        Apply the transformation.\n",
    "        Args:\n",
    "            image (PIL.Image): The input image.\n",
    "            labels (torch.Tensor): Bounding box labels in YOLO format \n",
    "                                   (class, x_center, y_center, width, height).\n",
    "        Returns:\n",
    "            image (PIL.Image): Transformed image.\n",
    "            labels (torch.Tensor): Adjusted labels after flipping.\n",
    "        \"\"\"\n",
    "        if random.random() < self.flip_prob:\n",
    "            # Flip the image vertically\n",
    "            image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            \n",
    "            # Adjust the bounding box labels for the flip\n",
    "            if len(labels) > 0:\n",
    "                labels[:, 2] = 1 - labels[:, 2]  # Invert y_center for vertical flip\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "class ComposeCustomTransforms:\n",
    "    def __init__(self, transforms):\n",
    "        \"\"\"\n",
    "        Initialize with a list of custom transformations.\n",
    "        Args:\n",
    "            transforms (list): A list of callable custom transforms.\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, labels):\n",
    "        \"\"\"\n",
    "        Apply each transform in the sequence.\n",
    "        Args:\n",
    "            image (PIL.Image): Input image.\n",
    "            labels (torch.Tensor): YOLO-style labels.\n",
    "        Returns:\n",
    "            image, labels: Transformed image and labels.\n",
    "        \"\"\"\n",
    "        for transform in self.transforms:\n",
    "            image, labels = transform(image, labels)\n",
    "        return image, labels\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batches of variable-size bounding box labels.\n",
    "    Args:\n",
    "        batch (list): A list of (image, labels) tuples.\n",
    "    Returns:\n",
    "        images (torch.Tensor): Stacked images of shape [batch_size, channels, height, width].\n",
    "        labels (list): A list of label tensors, each of shape [num_boxes, 5].\n",
    "    \"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack all images\n",
    "    labels = [item[1] for item in batch]  # Keep labels as a list\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "custom_geom_transform = ComposeCustomTransforms([\n",
    "    RandomHorizontalFlipWithBBox(flip_prob=0.5),\n",
    "    RandomVerticalFlipWithBBox(flip_prob=0.5)\n",
    "])\n",
    "\n",
    "rgb_only_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "])\n",
    "\n",
    "all_transform = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor(),  # Convert to tensor after applying augmentations\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # For RGB channels only\n",
    "])\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = YOLODataset(\n",
    "    rgb_dir=\"YOLO/images/train\",\n",
    "    dsm_dir=\"YOLO/dsm/train\",\n",
    "    hillshade_dir=\"YOLO/hillshade/train\",\n",
    "    label_dir=\"YOLO/labels/train\",\n",
    "    geom_transform=custom_geom_transform,\n",
    "    rgb_transform=rgb_only_transform,\n",
    "    all_transform=all_transform\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  \n",
    "    collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPEO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
